{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p2admin/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from tensorflow.contrib import rnn\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import dateutil.tz\n",
    "# from utils.utils import *\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "def creat_dir(network_type):\n",
    "    \"\"\"code from on InfoGAN\"\"\"\n",
    "    now = datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    root_log_dir = \"logs/\" + network_type\n",
    "    exp_name = network_type + \"_%s\" % timestamp\n",
    "    log_dir = os.path.join(root_log_dir, exp_name)\n",
    "\n",
    "    now = datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    root_model_dir = \"models/\" + network_type\n",
    "    exp_name = network_type + \"_%s\" % timestamp\n",
    "    model_dir = os.path.join(root_model_dir, exp_name)\n",
    "\n",
    "    for path in [log_dir, model_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    return log_dir, model_dir\n",
    "\n",
    "tic = time.clock()\n",
    "\n",
    "E0 = 1\n",
    "Emin = 1e-9\n",
    "nu = 0.3\n",
    "batch_size = 3\n",
    "\n",
    "'Input'\n",
    "nelx, nely, alpha, alpha2, gamma, rmin, density_r = 12*4, 4*4, 0.6, 0.6, 3.0, 3.0, 6.0\n",
    "\n",
    "'Algorithm parameters'\n",
    "p, nn, epsilon_al, epsilon_opt = 16,nely * nelx, 1, 1e-3\n",
    "\n",
    "'Prepare filter'\n",
    "r = rmin\n",
    "Range = np.arange(-r, r + 1)\n",
    "X = np.array([Range] * len(Range))\n",
    "Y = np.array([Range] * len(Range)).T\n",
    "X_temp = X.T.reshape(len(Range) ** 2, 1)\n",
    "Y_temp = Y.T.reshape(len(Range) ** 2, 1)\n",
    "neighbor = np.concatenate((X_temp, Y_temp), 1)\n",
    "D = np.sum(neighbor ** 2, 1)\n",
    "rn = sum(D <= r ** 2) - sum(D == 0)\n",
    "\n",
    "pattern_index = []\n",
    "for i in range(len(D)):\n",
    "    if D[i] <= r ** 2 and D[i] != 0:\n",
    "        pattern_index.append(i)\n",
    "pattern = neighbor[pattern_index]\n",
    "\n",
    "locX = np.asarray([np.arange(1, nelx + 1)] * nely)\n",
    "locY = np.asarray([np.arange(1, nely + 1)] * nelx).T\n",
    "loc = np.asarray([locY.T.reshape(-1), locX.T.reshape(-1)]).T\n",
    "\n",
    "M = np.zeros([nn, rn])\n",
    "for i in range(nn):\n",
    "    for j in range(rn):\n",
    "        locc = loc[i, :] + pattern[j, :]\n",
    "        count_true = 0\n",
    "        if locc[0] > nely: count_true = count_true + 1\n",
    "        if locc[1] > nelx: count_true = count_true + 1\n",
    "\n",
    "        if sum(locc < 1) + count_true == 0:\n",
    "            M[i, j] = locc[0] + (locc[1] - 1) * nely\n",
    "        else:\n",
    "            M[i, j] = 'NaN'\n",
    "\n",
    "iH = np.ones([int(nelx * nely * (2 * (np.ceil(rmin) - 1) + 1) ** 2), 1], dtype=int)\n",
    "jH = np.ones(iH.shape, dtype=int)\n",
    "sH = np.zeros(iH.shape)\n",
    "k = 0\n",
    "for i1 in range(nelx):\n",
    "    for j1 in range(nely):\n",
    "        e1 = (i1) * nely + (j1 + 1)\n",
    "        for i2 in np.arange(int(max(i1 + 1 - (np.ceil(rmin) - 1), 1)),\n",
    "                            int(min(i1 + 1 + (np.ceil(rmin) - 1), nelx)) + 1):\n",
    "            for j2 in np.arange(int(max(j1 + 1 - (np.ceil(rmin) - 1), 1)),\n",
    "                                int(min(j1 + 1 + (np.ceil(rmin) - 1), nely)) + 1):\n",
    "                e2 = (i2 - 1) * nely + j2\n",
    "                iH[k] = e1\n",
    "                jH[k] = e2\n",
    "                sH[k] = max(0, rmin - np.sqrt((i1 + 1 - i2) ** 2 + (j1 + 1 - j2) ** 2))\n",
    "                k = k + 1\n",
    "\n",
    "iH = iH.reshape(-1) - 1\n",
    "jH = jH.reshape(-1) - 1\n",
    "sH = sH.reshape(-1)\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "H = sps.csc_matrix((sH, (iH, jH))).toarray()\n",
    "Hs = np.sum(H, 1)\n",
    "bigM = H > 0\n",
    "\n",
    "'create neighbourhood index for N'\n",
    "r = density_r\n",
    "Range = np.arange(-r, r + 1)\n",
    "mesh = Range\n",
    "X = np.asarray([Range] * len(Range))\n",
    "Y = np.asarray([Range] * len(Range)).T\n",
    "neighbor = np.array([X.T.reshape(-1), Y.T.reshape(-1)]).T\n",
    "D = np.sum(neighbor ** 2, 1)\n",
    "rn = np.sum(D <= r ** 2)\n",
    "pattern = neighbor[D <= r ** 2, :]\n",
    "locX = np.asarray([np.arange(1, nelx + 1)] * nely)\n",
    "locY = np.asarray([np.arange(1, nely + 1)] * nelx).T\n",
    "loc = np.asarray([locY.T.reshape(-1), locX.T.reshape(-1)]).T\n",
    "N = np.zeros([nn, rn])\n",
    "for i in range(nn):\n",
    "    for j in range(rn):\n",
    "        locc = loc[i, :] + pattern[j, :]\n",
    "        count_true = 0\n",
    "        if locc[0] > nely: count_true = count_true + 1\n",
    "        if locc[1] > nelx: count_true = count_true + 1\n",
    "\n",
    "        if sum(locc < 1) + count_true == 0:\n",
    "            N[i, j] = locc[0] + (locc[1] - 1) * nely\n",
    "        else:\n",
    "            N[i, j] = 'NaN'\n",
    "\n",
    "idx_temp = (np.array([np.arange(nn)] * rn).T).reshape(-1).reshape(nn * rn, 1)\n",
    "N_t = N.T\n",
    "idy = []\n",
    "idx = []\n",
    "import math\n",
    "\n",
    "k = 0\n",
    "for i in range(N_t.shape[1]):\n",
    "    for j in range(N_t.shape[0]):\n",
    "        if not math.isnan(N_t[j, i]):\n",
    "            idy.append(N_t[j, i])\n",
    "            idx.append(idx_temp[k])\n",
    "        k = k + 1\n",
    "idy = np.asarray(idy).reshape(len(idy), 1) - 1\n",
    "idx = np.asarray(idx)\n",
    "bigN = sps.coo_matrix((np.ones(len(idx)), (idx.reshape(-1), idy.reshape(-1)))).toarray()\n",
    "N_count = np.sum(~np.isnan(N), axis=1)\n",
    "\n",
    "'Material Properties'\n",
    "E0, Emin, nu = 1, 1e-9, 0.3\n",
    "\n",
    "'PREPARE FINITE ELEMENT ANALYSIS'\n",
    "A11 = np.array([[12, 3, -6, -3], [3, 12, 3, 0], [-6, 3, 12, -3], [-3, 0, -3, 12]])\n",
    "A12 = np.array([[-6, -3, 0, 3], [-3, -6, -3, -6], [0, -3, -6, 3], [3, -6, 3, -6]])\n",
    "B11 = np.array([[-4, 3, -2, 9], [3, -4, -9, 4], [-2, -9, -4, -3], [9, 4, -3, -4]])\n",
    "B12 = np.array([[2, -3, 4, -9], [-3, 2, 9, -2], [4, 9, 2, 3], [-9, -2, 3, 2]])\n",
    "\n",
    "A1 = np.concatenate((A11, A12), axis=1)\n",
    "A2 = np.concatenate((A12.T, A11), axis=1)\n",
    "A = np.concatenate((A1, A2))\n",
    "\n",
    "B1 = np.concatenate((B11, B12), axis=1)\n",
    "B2 = np.concatenate((B12.T, B11), axis=1)\n",
    "B = np.concatenate((B1, B2))\n",
    "\n",
    "KE = 1 / (1 - nu ** 2) / 24. * (A + nu * B)\n",
    "nodenrs = np.arange((1 + nelx) * (1 + nely)).reshape(1 + nelx, 1 + nely).T\n",
    "edofVec = (2 * (nodenrs[0:-1, 0:-1] + 1)).reshape(nelx * nely, 1, order='F')\n",
    "edofMat = np.asarray([edofVec.reshape(-1)] * 8).T + \\\n",
    "          np.asarray(([0, 1, 2 * nely + 2, 2 * nely + 3, 2 * nely + 0, 2 * nely + 1, -2, -1]) * nelx * nely).reshape(\n",
    "              nelx * nely, 8)\n",
    "\n",
    "iK = np.zeros([edofMat.shape[0] * 8, edofMat.shape[1] * 1])\n",
    "for i in range(edofMat.shape[0]):\n",
    "    for j in range(edofMat.shape[1]):\n",
    "        iK[i * 8:(i + 1) * 8, j] = np.asarray(edofMat[i, j] * np.ones([8, 1])).reshape(-1)\n",
    "iK = (iK.astype(np.int32).T).reshape(64 * nelx * nely, 1, order='F')\n",
    "\n",
    "jK = np.zeros([edofMat.shape[0] * 1, edofMat.shape[1] * 8])\n",
    "for i in range(edofMat.shape[0]):\n",
    "    for j in range(edofMat.shape[1]):\n",
    "        jK[i, j * 8:(j + 1) * 8] = np.asarray(edofMat[i, j] * np.ones([1, 8])).reshape(-1)\n",
    "jK = (jK.astype(np.int32).T).reshape(64 * nelx * nely, 1, order='F')\n",
    "\n",
    "'DEFINE LOADS AND SUPPORTS (HALF MBB-BEAM)'\n",
    "# F = np.zeros([2 * (nely + 1) * (nelx + 1), 1])\n",
    "# F[((nely + 1) * nelx) * 2 + nely + 1, 0] = -1\n",
    "F = tf.placeholder(tf.float32, shape=([2*(nely+1)*(nelx+1),batch_size]))\n",
    "\n",
    "\n",
    "# F=sps.coo_matrix(F)\n",
    "U = np.zeros([2 * (nely + 1) * (nelx + 1), 1])\n",
    "# fixeddofs=(np.arange(0,2*(nely+1),2).tolist());fixeddofs.append(2*(nelx+1)*(nely+1)-1)\n",
    "# fixeddofs=np.asarray(fixeddofs).reshape(1,len(fixeddofs))\n",
    "# alldofs = np.arange(2*(nely+1)*(nelx+1)).reshape(1,2*(nely+1)*(nelx+1))\n",
    "\n",
    "fixeddofs = np.asarray(np.arange(0, 2 * (nely + 1), 1).tolist())\n",
    "fixeddofs = fixeddofs.reshape(1, len(fixeddofs))\n",
    "alldofs = np.arange(2 * (nely + 1) * (nelx + 1)).reshape(1, 2 * (nely + 1) * (nelx + 1))\n",
    "\n",
    "freedofs = np.setdiff1d(alldofs, fixeddofs)\n",
    "\n",
    "'prepare some stuff to reduce cost'\n",
    "dphi_idphi = (H / sum(H)).T\n",
    "\n",
    "'Start Iteration'\n",
    "# phi = alpha*np.ones([nn,1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "z_dim, h_dim_1, h_dim_2, h_dim_3, h_dim_4, h_dim_5 = 2, 100, 100, 100, 100, 100\n",
    "\n",
    "# r_lag = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "# r_lag2 = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "# lamda_lag = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "# lamda_lag2 = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "\n",
    "F_input = tf.placeholder(tf.float32, shape=([batch_size, z_dim]))\n",
    "# F = tf.placeholder(tf.float32, shape=([2*(nely+1)*(nelx+1),batch_size]))\n",
    "\n",
    "W1 = tf.Variable(xavier_init([z_dim, h_dim_1]))\n",
    "b1 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "W2 = tf.Variable(xavier_init([h_dim_1, h_dim_2]))\n",
    "b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "W3 = tf.Variable(xavier_init([h_dim_2, h_dim_3]))\n",
    "b3 = tf.Variable(tf.zeros(shape=[h_dim_3]))\n",
    "\n",
    "W4 = tf.Variable(xavier_init([h_dim_3, h_dim_4]))\n",
    "b4 = tf.Variable(tf.zeros(shape=[h_dim_4]))\n",
    "\n",
    "W5 = tf.Variable(xavier_init([h_dim_4, nn]))\n",
    "b5 = tf.Variable(tf.zeros(shape=[nn]))\n",
    "\n",
    "# W6 = tf.Variable(xavier_init([h_dim_5, nn]))\n",
    "# b6 = tf.Variable(tf.zeros(shape=[nn]))\n",
    "\n",
    "h1 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(F_input, W1) + b1),scale=True)\n",
    "h2 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h1, W2) + b2),scale=True)\n",
    "h3 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h2, W3) + b3),scale=True)\n",
    "h4 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h3, W4) + b4),scale=True)\n",
    "# h5 = tf.nn.relu(tf.matmul(h4, W5) + b5)\n",
    "    \n",
    "phi_ = tf.sigmoid(tf.matmul(h4, W5) + b5)\n",
    "phi = tf.reshape(phi_, [nn, batch_size])\n",
    "\n",
    "# phi = (tf.Variable(alpha * np.ones([nn, 1]), dtype='float32'))\n",
    "\n",
    "loop = 0\n",
    "# delta = 1.0\n",
    "\n",
    "# while beta < 1000:\n",
    "loop = loop + 1\n",
    "loop2 = 0\n",
    "'augmented lagrangian parameters'\n",
    "# r_lag = 1\n",
    "# r_lag2 = 0.001\n",
    "# lamda_lag = 0\n",
    "# lamda_lag2 = 0\n",
    "\n",
    "eta = 0.1\n",
    "eta2 = 0.1\n",
    "epsilon = 1\n",
    "dphi = 1e6\n",
    "\n",
    "r_lag = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "r_lag2 = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "lamda_lag = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "lamda_lag2 = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "beta = tf.placeholder(tf.float32, shape=([batch_size,1]))\n",
    "################################################################\n",
    "'get initial g and c'\n",
    "\n",
    "import time\n",
    "\n",
    "W1_fake = tf.Variable(xavier_init([z_dim, h_dim_1]), trainable=False)\n",
    "b1_fake = tf.Variable(tf.zeros(shape=[h_dim_1]), trainable=False)\n",
    "W1_fake = W1_fake.assign(W1)\n",
    "b1_fake = b1_fake.assign(b1)\n",
    "\n",
    "W2_fake = tf.Variable(xavier_init([h_dim_1, h_dim_2]), trainable=False)\n",
    "b2_fake = tf.Variable(tf.zeros(shape=[h_dim_2]), trainable=False)\n",
    "W2_fake = W2_fake.assign(W2)\n",
    "b2_fake = b2_fake.assign(b2)\n",
    "\n",
    "W3_fake = tf.Variable(xavier_init([h_dim_2, h_dim_3]), trainable=False)\n",
    "b3_fake = tf.Variable(tf.zeros(shape=[h_dim_3]), trainable=False)\n",
    "W3_fake = W3_fake.assign(W3)\n",
    "b3_fake = b3_fake.assign(b3)\n",
    "\n",
    "W4_fake = tf.Variable(xavier_init([h_dim_3, h_dim_4]), trainable=False)\n",
    "b4_fake = tf.Variable(tf.zeros(shape=[h_dim_4]), trainable=False)\n",
    "W4_fake = W4_fake.assign(W4)\n",
    "b4_fake = b4_fake.assign(b4)\n",
    "\n",
    "W5_fake = tf.Variable(xavier_init([h_dim_4, nn]), trainable=False)\n",
    "b5_fake = tf.Variable(tf.zeros(shape=[nn]), trainable=False)\n",
    "W5_fake = W5_fake.assign(W5)\n",
    "b5_fake = b5_fake.assign(b5)\n",
    "\n",
    "# W6_fake = tf.Variable(xavier_init([h_dim_5, nn]), trainable=False)\n",
    "# b6_fake = tf.Variable(tf.zeros(shape=[nn]), trainable=False)\n",
    "# W6_fake = W6_fake.assign(W6)\n",
    "# b6_fake = b6_fake.assign(b6)\n",
    "\n",
    "h1_fake = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(F_input, W1_fake) + b1_fake),scale=True,trainable=False)\n",
    "h2_fake = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h1_fake, W2_fake) + b2_fake),scale=True,trainable=False)\n",
    "h3_fake = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h2_fake, W3_fake) + b3_fake),scale=True,trainable=False)\n",
    "h4_fake = tf.contrib.layers.batch_norm(tf.nn.relu(tf.matmul(h3_fake, W4_fake) + b4_fake),scale=True,trainable=False)\n",
    "# h5_fake = tf.nn.relu(tf.matmul(h4_fake, W5_fake) + b5_fake)\n",
    "\n",
    "phi_temp_fake = tf.sigmoid(tf.matmul(h4_fake, W5_fake) + b5_fake)\n",
    "phi_fake = tf.reshape(phi_temp_fake,[nn,batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sep_grad_store,error_store,dphi_store,dobj_store=[],[],[],[]\n",
    "c,g,global_density=[],[],[]\n",
    "c_fake,g_fake,global_density_fake=[],[],[]\n",
    "rho,rho_fake=[],[]\n",
    "dphi_fake=[]\n",
    "\n",
    "tic=time.clock()\n",
    "\n",
    "learning_rate_fake = tf.placeholder(tf.float32,shape=([batch_size,1]))\n",
    "g_old = [tf.Variable(0, dtype='float32',trainable=False)]*batch_size\n",
    "global_density_old = [tf.Variable([[0]], dtype='float32',trainable=False)]*batch_size\n",
    "c_old = [tf.Variable(0, dtype='float32',trainable=False)]*batch_size\n",
    "tic=time.clock()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    phi_til = tf.matmul(tf.cast(H, tf.float32), phi[:,i:i+1]) / Hs.reshape(nn, 1)\n",
    "    rho.append((tf.tanh(beta[i][0] / 2.0) + tf.tanh(beta[i][0] * (phi_til - 0.5))) / (2 * tf.tanh(beta[i][0] / 2.0)))\n",
    "\n",
    "    sK_temp = tf.transpose((KE.reshape(KE.shape[0] * KE.shape[1], 1) * \\\n",
    "                            tf.reshape(Emin + tf.reshape(rho[i], [-1]) ** (gamma) * (E0 - Emin), [1, nelx * nely])))\n",
    "    sK = tf.reshape(sK_temp, [8 * 8 * nelx * nely, 1])\n",
    "\n",
    "    ###################\n",
    "    indices_m = np.stack((iK.reshape(-1), jK.reshape(-1)), axis=1)\n",
    "    values_m = tf.reshape(sK, [-1])\n",
    "\n",
    "    linearized_m = tf.matmul(indices_m, [[10000], [1]])\n",
    "    y_m, idx_m = tf.unique(tf.squeeze(linearized_m))\n",
    "\n",
    "    idx_m_sort, ind_m_sort = tf.nn.top_k(idx_m, k=nelx * nely * 64)\n",
    "    idx_m_sort = tf.reverse(idx_m_sort, [0])\n",
    "    ind_m_sort = tf.reverse(ind_m_sort, [0])\n",
    "\n",
    "    # values_m_test = values_m\n",
    "    values_m = tf.gather(values_m, ind_m_sort)\n",
    "    values_m = tf.segment_sum(values_m, idx_m_sort)\n",
    "\n",
    "    y_m = tf.expand_dims(y_m, 1)\n",
    "    indices_m = tf.concat([y_m // 10000, y_m % 10000], axis=1)\n",
    "    #####################\n",
    "\n",
    "    #####################\n",
    "    K_sp = tf.SparseTensor(tf.cast(indices_m, tf.int64),\n",
    "                           tf.reshape(tf.cast(values_m, tf.float32), [-1]),\n",
    "                           [(nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2])\n",
    "    K_sp = tf.sparse_add(tf.zeros(((nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2)), K_sp)\n",
    "    K_dense = (K_sp + tf.transpose(K_sp)) / 2\n",
    "    #####################\n",
    "\n",
    "    K_temp = tf.gather(K_dense, freedofs.astype(np.int32))\n",
    "    K_free = tf.transpose(tf.gather(tf.transpose(K_temp), freedofs.astype(np.int32)))\n",
    "    \n",
    "    F_RHS=tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),\n",
    "                              tf.float32)\n",
    "    chol = tf.cholesky(K_free+np.diag((np.ones([1,(nelx+1)*(nely+1)*2-len(fixeddofs[0])])*1e-8).tolist()[0]))\n",
    "    U_pre = tf.cholesky_solve(chol,F_RHS)\n",
    "    \n",
    "#     U_pre = tf.matmul(tf.matrix_inverse(tf.cast(K_free, tf.float32)),\n",
    "#                       tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),\n",
    "#                               tf.float32))\n",
    "\n",
    "    U = tf.sparse_add(tf.zeros(((nelx + 1) * (nely + 1) * 2)),\n",
    "                      tf.SparseTensor(freedofs.reshape((nelx + 1) * (nely + 1) * 2 - (nely + 1) * 2, 1),\n",
    "                      tf.reshape(tf.cast(U_pre, tf.float32), [-1]), [(nelx + 1) * (nely + 1) * 2]))\n",
    "\n",
    "    'Objective Function and Sensitivity Analysis'\n",
    "    ce = tf.reduce_sum(tf.multiply(tf.matmul(tf.reshape(tf.gather(U, edofMat.astype(np.int32)), edofMat.shape),\n",
    "                                             KE.astype(np.float32)), tf.gather(U, edofMat.astype(np.int32))), axis=1)\n",
    "    ce = tf.reshape(ce, [nn, 1])\n",
    "    c.append(tf.reduce_sum(tf.multiply(tf.cast(tf.multiply(rho[i] ** gamma, (E0 - Emin)), tf.float32), ce)))\n",
    "    rho_bar = tf.divide(tf.matmul(tf.cast(bigN, tf.float32), rho[i]), N_count.reshape(nn, 1))\n",
    "    g.append((tf.reduce_sum(rho_bar ** p) / nely / nelx) ** (1.0 / p) / alpha - 1.0)\n",
    "    global_density.append(tf.matmul(tf.transpose(rho[i]), tf.ones([nn, 1])) / nn - alpha2)\n",
    "    ###################################################################\n",
    "\n",
    "    # if ((max(abs(dphi)))<epsilon_al and g<0.005 and loo2>2) or (loo2>100):\n",
    "\n",
    "\n",
    "    dc_drho = -gamma * rho[i] ** (gamma - 1) * (E0 - Emin) * ce\n",
    "    drho_dphi = beta[i][0] * (1 - tf.tanh(beta[i][0] * (phi[:,i:i+1] - 0.5)) ** 2) / 2.0 / tf.tanh(beta[i][0] / 2.)\n",
    "    dc_dphi = tf.reduce_sum(bigM * (dphi_idphi * (dc_drho * drho_dphi)), axis=0)\n",
    "    dg_drhobar = 1.0 / alpha / nn * (1.0 / nn * tf.reduce_sum(rho_bar ** p)) ** (1.0 / p - 1) * rho_bar ** (p - 1)\n",
    "    dg_dphi = tf.reduce_sum(bigM * (dphi_idphi * (tf.matmul(tf.cast(bigN, tf.float32),\n",
    "                                                            (dg_drhobar / N_count.reshape(nn, 1))) * drho_dphi)), axis=0)\n",
    "\n",
    "    dphi = dc_dphi + tf.cast(lamda_lag[i][0] * dg_dphi * tf.reduce_sum(tf.cast(g[i] > 0, tf.float32)), tf.float32) + \\\n",
    "           tf.cast(2.0 / r_lag[i][0] * g[i] * dg_dphi * tf.reduce_sum(tf.cast(g[i] > 0, tf.float32)), tf.float32) + \\\n",
    "           lamda_lag2[i][0] * tf.ones(nn, 1) / nn * (tf.reduce_sum(tf.cast(global_density[i] > 0, tf.float32))) + \\\n",
    "           tf.reshape(2.0 / r_lag2[i][0] * global_density[i] / nn / nn * tf.ones([nn, 1]) * (\n",
    "           tf.reduce_sum(tf.cast(global_density[i] > 0, tf.float32))), [-1])\n",
    "    \n",
    "    dphi_store.append(dphi)\n",
    "    \n",
    "\n",
    "    \n",
    "############################################################################################################\n",
    "\n",
    "    'get initial g and c'\n",
    "    phi_til_fake = tf.matmul(tf.cast(H, tf.float32), phi_fake[:,i:i+1]) / Hs.reshape(nn, 1)\n",
    "    rho_fake.append((tf.tanh(beta[i][0] / 2.0) + tf.tanh(beta[i][0] * (phi_til_fake - 0.5))) / (2 * tf.tanh(beta[i][0] / 2.0)))\n",
    "\n",
    "    sK_temp_fake = tf.transpose((KE.reshape(KE.shape[0] * KE.shape[1], 1) * \\\n",
    "                                 tf.reshape(Emin + tf.reshape(rho_fake[i], [-1]) ** (gamma) * (E0 - Emin), [1, nelx * nely])))\n",
    "    sK_fake = tf.reshape(sK_temp_fake, [8 * 8 * nelx * nely, 1])\n",
    "\n",
    "    ###################\n",
    "    indices_m = np.stack((iK.reshape(-1), jK.reshape(-1)), axis=1)\n",
    "    values_m_fake = tf.reshape(sK_fake, [-1])\n",
    "\n",
    "    linearized_m = tf.matmul(indices_m, [[10000], [1]])\n",
    "    y_m, idx_m = tf.unique(tf.squeeze(linearized_m))\n",
    "\n",
    "    idx_m_sort, ind_m_sort = tf.nn.top_k(idx_m, k=nelx * nely * 64)\n",
    "    idx_m_sort = tf.reverse(idx_m_sort, [0])\n",
    "    ind_m_sort = tf.reverse(ind_m_sort, [0])\n",
    "\n",
    "    values_m_fake = tf.gather(values_m_fake, ind_m_sort)\n",
    "    values_m_fake = tf.segment_sum(values_m_fake, idx_m_sort)\n",
    "\n",
    "    y_m = tf.expand_dims(y_m, 1)\n",
    "    indices_m = tf.concat([y_m // 10000, y_m % 10000], axis=1)\n",
    "    #####################\n",
    "\n",
    "    #####################\n",
    "    K_sp_fake = tf.SparseTensor(tf.cast(indices_m, tf.int64),\n",
    "                                tf.reshape(tf.cast(values_m_fake, tf.float32), [-1]),\n",
    "                                [(nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2])\n",
    "    K_sp_fake = tf.sparse_add(tf.zeros(((nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2)), K_sp_fake)\n",
    "    K_dense_fake = (K_sp_fake + tf.transpose(K_sp_fake)) / 2.0\n",
    "    #####################\n",
    "\n",
    "    K_temp_fake = tf.gather(K_dense_fake, freedofs.astype(np.int32))\n",
    "    K_free_fake = tf.transpose(tf.gather(tf.transpose(K_temp_fake), freedofs.astype(np.int32)))\n",
    "\n",
    "    F_RHS_fake=tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),\n",
    "                              tf.float32)\n",
    "    chol_fake = tf.cholesky(K_free_fake+np.diag((np.ones([1,(nelx+1)*(nely+1)*2-len(fixeddofs[0])])*1e-8).tolist()[0]))\n",
    "    U_pre_fake = tf.cholesky_solve(chol_fake,F_RHS_fake)\n",
    "    \n",
    "#     U_pre_fake = tf.matmul(tf.matrix_inverse(tf.cast(K_free_fake, tf.float32)),\n",
    "#                            tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),\n",
    "#                                    tf.float32))\n",
    "\n",
    "    U_fake = tf.sparse_add(tf.zeros(((nelx + 1) * (nely + 1) * 2)),\n",
    "                           tf.SparseTensor(freedofs.reshape((nelx + 1) * (nely + 1) * 2 - (nely + 1) * 2, 1),\n",
    "                                           tf.reshape(tf.cast(U_pre_fake, tf.float32), [-1]),\n",
    "                                           [(nelx + 1) * (nely + 1) * 2]))\n",
    "\n",
    "    'Objective Function and Sensitivity Analysis'\n",
    "    ce_fake = tf.reduce_sum(tf.multiply(tf.matmul(tf.reshape(tf.gather(U_fake, edofMat.astype(np.int32)), edofMat.shape),\n",
    "                                                  KE.astype(np.float32)), tf.gather(U_fake, edofMat.astype(np.int32))),\n",
    "                            axis=1)\n",
    "    ce_fake = tf.reshape(ce_fake, [nn, 1])\n",
    "    c_fake.append(tf.reduce_sum(tf.multiply(tf.cast(tf.multiply(rho_fake[i] ** gamma, (E0 - Emin)), tf.float32), ce_fake)))\n",
    "    rho_bar_fake=tf.divide(tf.matmul(tf.cast(bigN, tf.float32), rho_fake[i]), N_count.reshape(nn, 1))\n",
    "    g_fake.append((tf.reduce_sum(rho_bar_fake ** p) / nely / nelx) ** (1. / p) / alpha - 1.0)\n",
    "    global_density_fake.append(tf.matmul(tf.transpose(rho_fake[i]), tf.ones([nn, 1])) / nn - alpha2)\n",
    "    ###################################################################\n",
    "\n",
    "    # if ((max(abs(dphi)))<epsilon_al and g<0.005 and loo2>2) or (loo2>100):\n",
    "\n",
    "\n",
    "    dc_drho_fake = -gamma * rho_fake[i] ** (gamma - 1) * (E0 - Emin) * ce_fake\n",
    "    drho_dphi_fake = beta[i][0] * (1 - tf.tanh(beta[i][0] * (phi_fake[:,i:i+1] - 0.5)) ** 2) / 2.0 / tf.tanh(beta[i][0] / 2.)\n",
    "    dc_dphi_fake = tf.reduce_sum(bigM * (dphi_idphi * (dc_drho_fake * drho_dphi_fake)), axis=0)\n",
    "    dg_drhobar_fake = 1.0 / alpha / nn * (1.0 / nn * tf.reduce_sum(rho_bar_fake ** p)) ** (1.0 / p - 1) * rho_bar_fake ** (\n",
    "    p - 1)\n",
    "    dg_dphi_fake = tf.reduce_sum(bigM * (dphi_idphi * (tf.matmul(tf.cast(bigN, tf.float32),\n",
    "                                                                 (dg_drhobar_fake / N_count.reshape(nn,\n",
    "                                                                                                    1))) * drho_dphi_fake)),\n",
    "                                 axis=0)\n",
    "\n",
    "    A = dc_dphi_fake + tf.cast(lamda_lag[i][0] * dg_dphi_fake * tf.reduce_sum(tf.cast(g_fake[i] > 0, tf.float32)), tf.float32) + \\\n",
    "        tf.cast(2.0 / r_lag[i][0] * g_fake[i] * dg_dphi_fake * tf.reduce_sum(tf.cast(g_fake[i] > 0, tf.float32)), tf.float32)\n",
    "\n",
    "    B = lamda_lag2[i][0] * tf.ones(nn, 1) / nn * (tf.reduce_sum(tf.cast(global_density_fake[i] > 0, tf.float32))) + \\\n",
    "        2.0 / r_lag2[i][0] * global_density_fake[i] / nn / nn * tf.ones([nn]) * (tf.reduce_sum(tf.cast(global_density_fake[i] > 0, tf.float32)))\n",
    "\n",
    "    dphi_fake.append(A + B)\n",
    "    \n",
    "    \n",
    "    error_sep = tf.reduce_sum(((tf.reshape(dphi_fake[i], [nn, 1]) * phi[:,i:i+1])))\n",
    "    error_store.append(error_sep)\n",
    "    \n",
    "    sep_grad=tf.reduce_sum(tf.abs(tf.gradients((tf.reshape(dphi_fake[i], [nn, 1]) * phi[:,i:i+1]),W1)))\n",
    "    sep_grad_store.append(sep_grad)\n",
    "    \n",
    "############################################################################################\n",
    "#     g_old = tf.Variable(0, dtype='float32',trainable=False)\n",
    "#     global_density_old = tf.Variable([[0]], dtype='float32',trainable=False)\n",
    "#     c_old = tf.Variable(0, dtype='float32',trainable=False)\n",
    "\n",
    "    g_old[i] = g_old[i].assign(g[i])\n",
    "    global_density_old[i] = global_density_old[i].assign(global_density[i])\n",
    "    c_old[i] = c_old[i].assign(c[i])\n",
    "\n",
    "#     assign_g=tf.assign(g_old[i],g[i])\n",
    "#     assign_gd=tf.assign(global_density_old[i],global_density[i])\n",
    "#     assign_c=tf.assign(c_old[i],c[i])\n",
    "\n",
    "    'learning rate adjusting'\n",
    "    delta = -dphi_fake[i] * learning_rate_fake[i][0]\n",
    "    phi_temp = phi_fake[:,i:i+1] + tf.reshape(delta, [nn, 1])\n",
    "\n",
    "    # assign_phi = tf.assign(phi,phi_temp)\n",
    "\n",
    "    # phi_temp=tf.Variable(np.ones([nn,1]), dtype='float32')\n",
    "    # phi_temp=phi_temp.assign(phi)\n",
    "\n",
    "    phi_til_temp = tf.matmul(tf.cast(H, tf.float32), phi_temp) / tf.reshape(tf.cast(Hs, tf.float32), [nn, 1])\n",
    "    rho_temp = (tf.tanh(beta[i][0] / 2.0) + tf.tanh(beta[i][0] * (phi_til_temp - 0.5))) / (2 * tf.tanh(beta[i][0] / 2.0))\n",
    "    rho_bar_temp = (tf.matmul(tf.cast(bigN, tf.float32), rho_temp) / tf.reshape(tf.cast(N_count, tf.float32), [nn, 1]))\n",
    "    g_temp = (tf.reduce_sum(rho_bar_temp ** p) / nn) ** (1.0 / p) / alpha - 1.0\n",
    "    global_density_temp = tf.matmul(tf.transpose(rho_temp), tf.ones([nn, 1])) / nn - alpha2\n",
    "\n",
    "    sK_temp_fake2 = tf.transpose((KE.reshape(KE.shape[0] * KE.shape[1], 1) * \\\n",
    "                                  tf.reshape(Emin + tf.reshape(rho_temp, [-1]) ** (gamma) * (E0 - Emin), [1, nelx * nely])))\n",
    "    sK_fake2 = tf.reshape(sK_temp_fake2, [8 * 8 * nelx * nely, 1])\n",
    "\n",
    "    ###################\n",
    "    values_m_fake2 = tf.reshape(sK_fake2, [-1])\n",
    "    values_m_fake2 = tf.gather(values_m_fake2, ind_m_sort)\n",
    "    values_m_fake2 = tf.segment_sum(values_m_fake2, idx_m_sort)\n",
    "    ###################\n",
    "\n",
    "    ###################\n",
    "    K_sp_fake2 = tf.SparseTensor(tf.cast(indices_m, tf.int64),\n",
    "                                 tf.reshape(tf.cast(values_m_fake2, tf.float32), [-1]),\n",
    "                                 [(nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2])\n",
    "    K_sp_fake2 = tf.sparse_add(tf.zeros(((nely + 1) * (nelx + 1) * 2, (nely + 1) * (nelx + 1) * 2)), K_sp_fake2)\n",
    "    K_dense_fake2 = (K_sp_fake2 + tf.transpose(K_sp_fake2)) / 2.0\n",
    "    ###################\n",
    "\n",
    "    K_temp_fake2 = tf.gather(K_dense_fake2, freedofs.astype(np.int32))\n",
    "    K_free_fake2 = tf.transpose(tf.gather(tf.transpose(K_temp_fake2), freedofs.astype(np.int32)))\n",
    "    \n",
    "    F_RHS_fake2=tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),\n",
    "                              tf.float32)\n",
    "    chol_fake2 = tf.cholesky(K_free_fake2+np.diag((np.ones([1,(nelx+1)*(nely+1)*2-len(fixeddofs[0])])*1e-8).tolist()[0]))\n",
    "    U_pre_fake2 = tf.cholesky_solve(chol_fake2,F_RHS_fake2)\n",
    "    \n",
    "#     U_pre_fake2 = tf.matmul(tf.matrix_inverse(tf.cast(K_free_fake2, tf.float32)+np.diag((np.ones([1,(nelx+1)*(nely+1)*2-len(fixeddofs[0])])*1e-15).tolist()[0])),\n",
    "#                             tf.cast(tf.gather(tf.reshape(F[:,i:i+1], [(nelx + 1) * (nely + 1) * 2, 1]), freedofs.astype(np.int64)),tf.float32))\n",
    "\n",
    "    U_fake2 = tf.sparse_add(tf.zeros(((nelx + 1) * (nely + 1) * 2)),\n",
    "                            tf.SparseTensor(freedofs.reshape((nelx + 1) * (nely + 1) * 2 - (nely + 1) * 2, 1),\n",
    "                                            tf.reshape(tf.cast(U_pre_fake2, tf.float32), [-1]),\n",
    "                                            [(nelx + 1) * (nely + 1) * 2]))\n",
    "\n",
    "    ce_temp = tf.reduce_sum(tf.multiply(tf.matmul(tf.reshape(tf.gather(U_fake2, edofMat.astype(np.int32)), edofMat.shape),\n",
    "              KE.astype(np.float32)), tf.gather(U_fake2, edofMat.astype(np.int32))),axis=1)\n",
    "    \n",
    "    ce_temp = tf.reshape(ce_temp, [nn, 1])\n",
    "    c_temp = tf.reduce_sum(tf.multiply(tf.cast(tf.multiply(rho_temp ** gamma, (E0 - Emin)), tf.float32), ce_temp))\n",
    "\n",
    "    A2 = (c_temp + lamda_lag[i][0] * g_temp * (tf.reduce_sum(tf.cast(g_temp > 0, tf.float32))) + 1.0 / r_lag[i][0] * g_temp ** 2 * (\n",
    "    tf.reduce_sum(tf.cast(g_temp > 0, tf.float32))) + \\\n",
    "          1.0 / r_lag2[i][0] * global_density_temp ** 2 * (tf.reduce_sum(tf.cast(global_density_temp > 0, tf.float32))))\n",
    "\n",
    "    B2 = (c_old[i] + lamda_lag[i][0] * g_old[i] * (tf.reduce_sum(tf.cast(g_old[i] > 0, tf.float32))) + 1.0 / r_lag[i][0] * g_old[i] ** 2 * (\n",
    "    tf.reduce_sum(tf.cast(g_old[i] > 0, tf.float32))) + \\\n",
    "          1.0 / r_lag2[i][0] * global_density_old[i] ** 2 * (tf.reduce_sum(tf.cast(global_density_old[i] > 0, tf.float32))))\n",
    "    dobj = A2 - B2\n",
    "    dobj_store.append(dobj)\n",
    "    \n",
    "toc=time.clock()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 1.0, staircase=True)\n",
    "\n",
    "solver0 = tf.train.AdamOptimizer(learning_rate).minimize(error_store[0], global_step=global_step)\n",
    "solver1 = tf.train.AdamOptimizer(learning_rate).minimize(error_store[1], global_step=global_step)\n",
    "solver2 = tf.train.AdamOptimizer(learning_rate).minimize(error_store[2], global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "force=-1\n",
    "F_batch=np.zeros([batch_size,z_dim])\n",
    "\n",
    "# alpha=1.5\n",
    "theta = np.linspace(np.pi*0.,2.0*np.pi/5.,batch_size)\n",
    "\n",
    "count=0\n",
    "for i in range(batch_size-1,-1,-1):\n",
    "#     theta = 2*(i+1)*np.pi/10.#+i*np.pi/9*alpha\n",
    "    print theta[i]\n",
    "#     theta = (i+1)*np.pi/10\n",
    "    Fx = force*np.sin(theta[i])\n",
    "    Fy = force*np.cos(theta[i])\n",
    "    \n",
    "    # up-right corner\n",
    "    F_batch[count,0]=Fx\n",
    "    F_batch[count,1]=Fy\n",
    "    count=count+1   \n",
    "\n",
    "count=0\n",
    "F_sp=np.zeros([2*(nely+1)*(nelx+1),batch_size])\n",
    "for i in range(batch_size-1,-1,-1):\n",
    "#     theta = 2*(i+1)*np.pi/10.#+i*np.pi/9*alpha\n",
    "#     theta = (i+1)*np.pi/5\n",
    "    Fx = force*np.sin(theta[i])\n",
    "    Fy = force*np.cos(theta[i])    \n",
    "    F_sp[(nely+1)*2*nelx+nely, count] = Fx\n",
    "    F_sp[(nely+1)*2*nelx+nely+1, count] = Fy\n",
    "    count=count+1\n",
    "\n",
    "print F_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamda_value = np.zeros([batch_size, 1])\n",
    "lamda_value2 = np.zeros([batch_size, 1])\n",
    "r_value = np.ones([batch_size, 1]) * 1\n",
    "r_value2 = np.ones([batch_size, 1]) * 0.001\n",
    "eta = np.ones([batch_size, 1]) * 0.1\n",
    "eta2 = np.ones([batch_size, 1]) * 0.1\n",
    "j, j2 = 0, 0\n",
    "\n",
    "k_j = np.ones([batch_size, 1])\n",
    "thre_range = 0.02\n",
    "agg = 0\n",
    "total_E_pre = 1e20\n",
    "count_k = 0\n",
    "count = 0\n",
    "drho=1e6\n",
    "it=0\n",
    "beta_value=np.ones([batch_size,1])*10\n",
    "learning_rate_fake_value=np.ones([batch_size,1])*0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S_bn\")\n",
    "if not os.path.exists(timestr):\n",
    "    os.makedirs(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# logdir,modeldir=creat_dir('infill_var')\n",
    "# summary_writer = tf.summary.FileWriter(logdir)\n",
    "# summary_op_train = tf.summary.merge([tf.summary.scalar(\"train/error\",error_total)])\n",
    "####################\n",
    "\n",
    "# for it in range(100000):\n",
    "it=0\n",
    "drho_total_store=[]\n",
    "\n",
    "while min(beta_value)<11:\n",
    "    it=it+1\n",
    "    # sess.run([assign_phi_fake,assign_c,assign_g,assign_gd], feed_dict={learning_rate_fake: starter_learning_rate, lamda_lag: lamda_value,\n",
    "    #                                 lamda_lag2: lamda_value2,\n",
    "    #                                 r_lag: r_value,\n",
    "    #                                 r_lag2: r_value2})\n",
    "    \n",
    "#     total_E, g_value, gd, dphi_fake_value, dobj_value, lr, phi_fake_value,rho_value = sess.run([error_store, g,\n",
    "#                                                                       global_density, dphi_fake, dobj_store,\n",
    "#                                                                       learning_rate,phi_fake,rho],\n",
    "#                                                                      feed_dict={learning_rate_fake: learning_rate_fake_value,\n",
    "#                                                                                 lamda_lag:lamda_value,\n",
    "#                                                                                 lamda_lag2:lamda_value2,\n",
    "#                                                                                 r_lag:r_value,\n",
    "#                                                                                 r_lag2:r_value2,\n",
    "#                                                                                 F_input:F_batch,\n",
    "#                                                                                 F:F_sp, \n",
    "#                                                                                 learning_rate:starter_learning_rate,beta:beta_value})\n",
    "\n",
    "    \n",
    "    \n",
    "    loop2=0\n",
    "        \n",
    "    while 1:\n",
    "#         try:\n",
    "        g_value_temp,dphi_fake_value,rho_pre = sess.run([g,dphi_fake,rho], \n",
    "                                              feed_dict={learning_rate_fake: learning_rate_fake_value,\n",
    "                                                         lamda_lag:lamda_value,lamda_lag2:lamda_value2,\n",
    "                                                         r_lag:r_value,r_lag2:r_value2,F_input: F_batch,\n",
    "                                                         F:F_sp,learning_rate:starter_learning_rate,\n",
    "                                                         beta:beta_value})\n",
    "#         except:\n",
    "#             stop=1\n",
    "\n",
    "        \n",
    "        dphi_fake_value_max=max(max(abs(np.array(dphi_fake_value[0])).reshape(-1)),\n",
    "                                max(abs(np.array(dphi_fake_value[1])).reshape(-1)),\n",
    "                                max(abs(np.array(dphi_fake_value[2])).reshape(-1)))\n",
    "        \n",
    "        \n",
    "        if (dphi_fake_value_max < 1*batch_size and max(g_value_temp) < 0.01 and loop2 > 0) or (loop2 > 100):\n",
    "\n",
    "#         if (drho < 0.05 and max(g_value_temp) < 0.005 and loop2 > 0) or (loop2 > 100):\n",
    "            #             print('wrong')\n",
    "            lamda_value = np.zeros([batch_size, 1])\n",
    "            lamda_value2 = np.zeros([batch_size, 1])\n",
    "            r_value = np.ones([batch_size, 1]) * 1\n",
    "            r_value2 = np.ones([batch_size, 1]) * 0.001\n",
    "            eta = np.ones([batch_size, 1]) * 0.1\n",
    "            eta2 = np.ones([batch_size, 1]) * 0.1\n",
    "            dphi_fake_value=np.array([1e6]*batch_size)\n",
    "            drho=np.array([1e6]*batch_size)\n",
    "            break\n",
    "\n",
    "        loop2 = loop2 + 1\n",
    "        j, j2 = 0, 0\n",
    "        loop3 = np.array([0]*batch_size)\n",
    "        dphi_fake_value = np.array([1e6]*batch_size)\n",
    "        drho = np.array([1e6]*batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "        while (max(max(abs(np.array(dphi_fake_value[0])).reshape(-1)),\n",
    "                   max(abs(np.array(dphi_fake_value[1])).reshape(-1)),\n",
    "                   max(abs(np.array(dphi_fake_value[2])).reshape(-1)))) > epsilon and min(loop3) < 1000:\n",
    "            \n",
    "#             rho_value_pre = sess.run(rho, feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,lamda_lag2:lamda_value2,r_lag:r_value,r_lag2:r_value2,\n",
    "#                                  F_input: F_batch,F:F_sp,learning_rate:starter_learning_rate,beta:beta_value})\n",
    "\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if count>=4000:\n",
    "                    break\n",
    "                if loop3[i]>1000:\n",
    "                    continue\n",
    "#                 dphi_fake_value[i] = np.array([1e6])\n",
    "#                 g_value = g_value_temp[i]\n",
    "\n",
    "                starter_learning_rate = 0.001\n",
    "                learning_rate_fake_value[i]=0.001\n",
    "                loop3[i] = loop3[i] + 1\n",
    "                loop4 = np.asarray([0]*batch_size)\n",
    "\n",
    "                tic=time.clock()\n",
    "                while loop4[i] < 10:\n",
    "                    try:\n",
    "                        dphi_fake_value, dobj_value,rho_value = sess.run([dphi_fake, dobj_store,rho],\n",
    "                                                   feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "                                                            lamda_lag2:lamda_value2, r_lag:r_value,\n",
    "                                                            r_lag2:r_value2,F_input:F_batch,\n",
    "                                                            F:F_sp,learning_rate:learning_rate_fake_value[i][0],\n",
    "                                                            beta:beta_value})\n",
    "                    except:\n",
    "                        starter_learning_rate = starter_learning_rate*0.5\n",
    "                        learning_rate_fake_value[i] = learning_rate_fake_value[i]*0.5\n",
    "                        \n",
    "\n",
    "\n",
    "                    if dobj_value[i] > 0:\n",
    "                        starter_learning_rate = starter_learning_rate * 0.5\n",
    "                        learning_rate_fake_value[i] = learning_rate_fake_value[i]*0.5\n",
    "                        loop4[i] = loop4[i] + 1\n",
    "                        if loop4[i] == 10:\n",
    "                            loop3[i] = 10000\n",
    "                            dphi_fake_value[i] = np.array([0])\n",
    "                            drho[i] = np.array([0])\n",
    "                            #                         break\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            try:\n",
    "                                sess.run([solver0],feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "                                                           lamda_lag2:lamda_value2,r_lag:r_value,r_lag2:r_value2,\n",
    "                                                           F_input: F_batch,F:F_sp,learning_rate:learning_rate_fake_value[i][0],beta:beta_value})\n",
    "\n",
    "                            except:\n",
    "                                print('jump0')\n",
    "                                \n",
    "                        if i == 1:\n",
    "                            try:\n",
    "                                sess.run([solver1],feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "                                                           lamda_lag2:lamda_value2,r_lag:r_value,r_lag2:r_value2,\n",
    "                                                           F_input: F_batch,F:F_sp,learning_rate:learning_rate_fake_value[i][0],beta:beta_value})\n",
    "\n",
    "                            except:\n",
    "                                print('jump1')\n",
    "                                \n",
    "                        if i == 2:\n",
    "                            try:\n",
    "                                sess.run([solver2],feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "                                                           lamda_lag2:lamda_value2,r_lag:r_value,r_lag2:r_value2,\n",
    "                                                           F_input: F_batch,F:F_sp,learning_rate:learning_rate_fake_value[i][0],beta:beta_value})\n",
    "                            except:\n",
    "                                print('jump2')\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                toc=time.clock()\n",
    "                count = count + 1\n",
    "#                 try:\n",
    "                g_value,c_value,sep_grad_value,error_total_value,phi_value,rho_curr=sess.run([g,c,sep_grad_store,error_store,phi,rho], \n",
    "                    feed_dict={learning_rate_fake: learning_rate_fake_value, lamda_lag: lamda_value,\n",
    "                    lamda_lag2: lamda_value2,r_lag: r_value,\n",
    "                    r_lag2: r_value2, F_input: F_batch, F: F_sp, learning_rate: learning_rate_fake_value[i][0],\n",
    "                    beta: beta_value})\n",
    "#                 except:\n",
    "#                     print('jump3')\n",
    "\n",
    "                drho[i]=sum(abs(rho_curr[i]-rho_pre[i]).reshape(-1))\n",
    "                drho_total=sum(abs(np.array(rho_curr)-np.array(rho_pre)).reshape(-1))\n",
    "                drho_total_store.append(drho_total)\n",
    "                print('iteration:{}'.format(it))\n",
    "                print('updating_case:{}'.format(i))\n",
    "                print('num_update:{}'.format(count))\n",
    "                print('total_error:{}'.format(np.sum(np.array(error_total_value))))\n",
    "                print('g_value:{}'.format(g_value))\n",
    "                print('c_value:{}'.format(c_value))\n",
    "                print('log_r:{}'.format(np.log(r_value)))\n",
    "#                 print('max_abs_dphi:{}'.format(max(abs(dphi_fake_value.reshape(-1)))))\n",
    "                print('gradient_W1:{}'.format(sep_grad_value))\n",
    "                print('dobj_value:{}'.format(dobj_value))\n",
    "                #                                                                     r_lag2:r_value2,F_input: F_batch,F:F_sp,learning_rate:starter_learning_rate,beta:beta_value})))\n",
    "                print('dphi_fake_value_max:{}'.format((max(abs(dphi_fake_value[i].reshape(-1))))))\n",
    "#                 print('drho_fake_value_max:{}'.format(max(abs(rho_value[i].reshape(-1)-rho_value_pre[i].reshape(-1)))))\n",
    "                print('drho:{}'.format(drho))\n",
    "                print('loop3:{}'.format(loop3))\n",
    "                print\n",
    "                print('running time:{}'.format(toc-tic))\n",
    "                print()\n",
    "                if count%500 == 0:\n",
    "                    sio.savemat('{}/c_value.mat'.format(timestr), mdict={'c': np.array(c_value)})\n",
    "                    sio.savemat('{}/rho.mat'.format(timestr), mdict={'rho': np.array(rho_curr)})\n",
    "                    sio.savemat('{}/drho.mat'.format(timestr), mdict={'drho': np.array(drho_total_store)})\n",
    "                    fig, axs = plt.subplots(2, 5, figsize=(15, 4), facecolor='w', edgecolor='k')\n",
    "                    fig.subplots_adjust(hspace=.1, wspace=.001)\n",
    "\n",
    "                    axs = axs.ravel()\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        axs[i].imshow(1 - rho_curr[i].reshape([nelx, nely]).T, 'gray')\n",
    "                        axs[i].set_title('c={}'.format(c_value[i]))\n",
    "                    plt.savefig('{}/opt_{}.png'.format(timestr, str(count)))\n",
    "                    plt.close()\n",
    "                if count >= 4000:\n",
    "                    break\n",
    "#             drho1=max(rho_value_cur[0].reshape(-1)-rho_value_pre[0].reshape(-1))\n",
    "#             drho2=max(rho_value_cur[1].reshape(-1)-rho_value_pre[1].reshape(-1))\n",
    "#             drho3=max(rho_value_cur[2].reshape(-1)-rho_value_pre[2].reshape(-1))\n",
    "#             drho=max(drho1,drho2,drho3)\n",
    "        \n",
    "    \n",
    "#         g_value, gd, rho_value_cur = sess.run([g, global_density,rho], \n",
    "#                                      feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "#                                      lamda_lag2:lamda_value2,r_lag:r_value,\n",
    "#                                      r_lag2:r_value2,F_input: F_batch,F:F_sp,\n",
    "#                                      learning_rate:starter_learning_rate,beta:beta_value})\n",
    "        for i in range(batch_size):    \n",
    "            g_value, gd= sess.run([g, global_density], \n",
    "                                     feed_dict={learning_rate_fake: learning_rate_fake_value,lamda_lag:lamda_value,\n",
    "                                     lamda_lag2:lamda_value2,r_lag:r_value,\n",
    "                                     r_lag2:r_value2,F_input: F_batch,F:F_sp,\n",
    "                                     learning_rate:learning_rate_fake_value[i][0],beta:beta_value})            \n",
    "            if g_value[i] < eta[i]:\n",
    "                lamda_value[i] = lamda_value[i] + 2 * g_value[i] / r_value[i]\n",
    "                j = j + 1\n",
    "                eta[i] = eta[i] * 0.5\n",
    "            else:\n",
    "                r_value[i] = r_value[i] * 0.5\n",
    "                j = 0\n",
    "\n",
    "            if gd[i] < eta2[i]:\n",
    "                lamda_value2[i] = lamda_value2[i] + 2 * gd[i][0] / r_value2[i]\n",
    "                j2 = j2 + 1\n",
    "                eta2[i] = eta2[i] * 0.5\n",
    "            else:\n",
    "                r_value2[i] = r_value2[i] * 0.5\n",
    "                j2 = 0\n",
    "                \n",
    "    for i in range(batch_size):\n",
    "        beta_value[i] = 1.5 * beta_value[i]\n",
    "\n",
    "phi_value, rho_value = sess.run([phi, rho],feed_dict={learning_rate_fake: learning_rate_fake_value, lamda_lag: lamda_value,lamda_lag2: lamda_value2,r_lag: r_value,\n",
    "               r_lag2: r_value2, F_input: F_batch, F: F_sp, learning_rate: starter_learning_rate, beta: beta_value})\n",
    "\n",
    "print('finished')\n",
    "# sio.savemat('result/infill_phi_{}.mat'.format(count), mdict={'phi': phi_value})\n",
    "# sio.savemat('result/infill_rho_{}.mat'.format(count), mdict={'rho': rho_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# if not os.path.exists(timestr):\n",
    "#     os.makedirs(timestr)\n",
    "\n",
    "sio.savemat('{}/c_value.mat'.format(timestr), mdict={'c': np.array(c_value)})\n",
    "sio.savemat('{}/rho.mat'.format(timestr), mdict={'rho': np.array(rho_curr)})\n",
    "sio.savemat('{}/drho.mat'.format(timestr), mdict={'drho': np.array(drho_total_store)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi_value, rho_value = sess.run([phi, rho],feed_dict={learning_rate_fake: learning_rate_fake_value, lamda_lag: lamda_value,lamda_lag2: lamda_value2,r_lag: r_value,\n",
    "               r_lag2: r_value2, F_input: F_batch, F: F_sp, learning_rate: starter_learning_rate, beta: beta_value})\n",
    "\n",
    "sio.savemat('result/infill_phi_{}.mat'.format(count), mdict={'phi': phi_value})\n",
    "sio.savemat('result/infill_rho_{}.mat'.format(count), mdict={'rho': rho_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'training result'\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 4), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .1, wspace=.001)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    axs[i].imshow(1-rho_value[i].reshape([nelx,nely]).T,'gray')\n",
    "#     axs[i].set_title(str(250+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
